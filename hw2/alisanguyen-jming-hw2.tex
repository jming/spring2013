\documentclass[11pt]{article}
\usepackage{latexsym}
\usepackage{amssymb,amsmath}
\usepackage[pdftex]{graphicx}
% \usepackage{qtree}

\topmargin = -.75in \textwidth=7in \textheight=9.3in
\oddsidemargin = -.3in \evensidemargin = -.3in

\begin{document}
\begin{center}
\large
CS181 Assignment 2
\end{center}
Joy Ming and Alisa Nguyen (25 February 2013)\\

\begin{enumerate}
\setcounter{enumi}{0}

\item Perceptron activation rule
	\begin{enumerate}
	\item Bright-or-dark is not able to be recognized by a perceptron. One possibility is to have the weights of all $n=9$ pixels be initialized to $1/n=1/9$. Then, multiplying the value if the pixel is on (1) or off (0) with the weights will give something greater than 0.75 if at least 75\% pixels are on and will give something less than 0.25 if at least 75\% pixels are off. However, this function of returning positive when the value is greater than 0.75 and less than 0.25 is piecewise and not differentiable. There cannot exist a perceptron that recognizes this feature because this feature is not linearly separable.
	% This perceptron would have the weights of all $n=9$ pixels to be initialzed to $1/n=1/9$. Then, multiplying the value if the pixel is on (1) or off (0) with the weights will give something greater than 0.75 if at least 75\% pixels are on and will give something less than 0.35 if at least 75\% pixels are off.
	\item Top-bright is able to be recognized by a perceptron. This perceptron would have $1/3$ weights for the top three pixels and $-1/6$ weights for the bottom pixels. If both have the same fraction of pixels on in both rows then the result should be 0. However, if the top has a larger fraction it would be positive and if the bottom has a larger fraction it would be negative. Therefore, simply checking to see if the result is greater than zero can tell if a larger fraction of pixels is on the top row than in the bottom two rows.
	\item It is not possible to present a perceptron that can check to see if a set of pixels that are on is connected because it is not possible to model when different points of data are correlated. Suppose for contradiction that there is a perceptron that could check to see if a set of pixels that are on is connected. Then for PROOF.
	\end{enumerate}
\item Four different possible learning algorithms
	\begin{enumerate}
	\item Decision trees
	\item Boosted decision stumps
	\item Perceptrons
	\item Multi-layer feed-forward neural networks
	\end{enumerate}
\item Implementing a neural network
\newline
\newline
5. We normalize the input values from [0, 255] to between 0 and 1 so we are able to better utilize the logistic sigmoid function.\\ 
\newline
6. Experiment wit simple networks
\begin{enumerate}
\item We used a learning rate of 0.1 based on this DATA:
\item 
\begin{enumerate}
\item Yes/No we are/are not in danger of overfitting by training for too many epochs.
\item A good number of ephocs to train for is
\\It is important that we use a validation set rather than the actual test set to tune the number of epochs because
\end{enumerate}
\item The training performance is
\\ The validation performance is
\\ The test performance is
\end{enumerate} 
7. Experiment with networks with a single hidden layer
\begin{enumerate}
\item We decided that we wanted to use a learning rate of FILLIN for 15 fully connected units in the hidden layer and FILLIN for 30 fully connected units in the hidden layer based on DATA.
\item We determine when to stop training by
\item GRAPH HERE
\item We used FILLIN epochs for 15 hidden units and FILLIN epochs for 30 hidden units.
\item Based on these experiements, we would use the network structure with FILLIN hidden units because FILLIN
\item The test set performance of the network we chose is FILLIN. This compares to the committee of perceptrons by FILLIN.
\end{enumerate}
8. CustomNetwork
\begin{enumerate}
\item We chose to implement
\item Our new network's test performance 
\end{enumerate}
\item Changing error function
	\begin{enumerate}
	\item Error function $C$ is implementing the idea of regularization, or that the less complex solution, or one that has less dimensions or degrees, is more desirable others that might work. In the new error function, different values are added onto the original loss function that will increase the sum depending on the increased complexity or layers of the network. This will penalize the loss function by squared weight values, making it more crucial to create less complicated graphs to minimize the loss function. 
	\item Our weight update rule for the error function $C$ derived from gradient descent is:
	\end{enumerate}

\end{enumerate}
\end{document}