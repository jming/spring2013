\documentclass[11pt]{article}
\usepackage{latexsym}
\usepackage{amssymb,amsmath}
\usepackage[pdftex]{graphicx}
% \usepackage{qtree}

\topmargin = -.75in \textwidth=7in \textheight=9.3in
\oddsidemargin = -.3in \evensidemargin = -.3in

\begin{document}
\begin{center}
\large
CS181 Assignment 2
\end{center}
Joy Ming and Alisa Nguyen (25 February 2013)\\

\begin{enumerate}
\setcounter{enumi}{0}

\item Perceptron activation rule
	\begin{enumerate}
	\item Bright-or-dark is able to be recognized by a perceptron. This perceptron would have the weights of all $n=9$ pixels to be initialzed to $1/n=1/9$. Then, multiplying the value if the pixel is on (1) or off (0) with the weights will give something greater than 0.75 if at least 75\% pixels are on and will give something less than 0.35 if at least 75\% pixels are off.
	\item Top-bright is also able to be recognized by a perceptron. This perceptron would have $1/3$ weights for the top three pixels and $-1/6$ weights for the bottom pixels. If both have the same fraction of pixels on in both rows then the result should be 0. However, if the top has a larger fraction it would be positive and if the bottom has a larger fraction it would be negative. Therefore, simply checking to see if the result is greater than zero can tell if a larger fraction of pixels is on the top row than in the bottom two rows.
	\item It is not possible to present a perceptron that can check to see if a set of pixels that are on is connected because it is not possible to model when different points of data are correlated. Suppose for contradiction that there is a perceptron that could check to see if a set of pixels that are on is connected. Then for PROOF.
	\end{enumerate}
\item Four different possible learning algorithms
	\begin{enumerate}
	\item Decision trees
	\item Boosted decision stumps
	\item Perceptrons
	\item Multi-layer feed-forward neural networks
	\end{enumerate}
\item Implementing a neural network
\newline
\newline
5. We normalize the input values from [0, 255] to between 0 and 1 so we are able to better utilize the logistic sigmoid function.
\item Changing error function
	\begin{enumerate}
	\item Error function $C$ is implementing the idea of regularization, or that the less complex solution, or one that has less dimensions or degrees, is more desirable others that might work. In the new error function, different values are added onto the original loss function that will increase the sum depending on the increased complexity or layers of the network. This will penalize the loss function by squared weight values, making it more crucial to create less complicated graphs to minimize the loss function. 
	\end{enumerate}

\end{enumerate}
\end{document}