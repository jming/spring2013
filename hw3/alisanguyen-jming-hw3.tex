\documentclass[11pt]{article}
\usepackage{latexsym}
\usepackage{amssymb,amsmath}
\usepackage[pdftex]{graphicx}
% \usepackage{qtree}
\usepackage{enumerate}

\topmargin = -.75in \textwidth=7in \textheight=9.3in
\oddsidemargin = -.3in \evensidemargin = -.3in

\begin{document}
\begin{center}
\large
CS181 Assignment 3
\end{center}
Joy Ming and Alisa Nguyen (15 March 2013)\\

\section{Problem 1}
\begin{enumerate}[a.]
\item The probability that all of the $M$ dimensions of $x - y$ are between $-\epsilon \text{ and } \epsilon$ is \fbox{$ \rho = (2\epsilon)^M$}.
\\ For each dimension $i$ of $\chi$, the probability that $|x_i - y_i| \leq \epsilon$ is equivalent to
\begin{center}
$P(|x_i - y_i| \leq \epsilon) = $ \\
$P(- \epsilon \leq x_i - y_i \leq \epsilon) = $ \\
$P(- \epsilon - x_i \leq - y_i \leq \epsilon - x_i) = $ \\
$P(\epsilon + x_i \geq y_i \geq x_i - \epsilon) = $ \\
$P(x_i - \epsilon \leq y_i \leq \epsilon + x_i)$
\end{center}
This distribution function is equivalent to $\int_{\epsilon + x_i}^{x_i - \epsilon} f(x)dx$, where $f(x)$ is the PDF of $y_i$, which we know to have a uniform distribution, so $f(x) = \frac{1}{b - a} = 1$. Thus, we get:
\begin{center}
$\int_{\epsilon + x_i}^{x_i - \epsilon} 1 dx = $ \\
$\epsilon + x_i - (x_i - \epsilon) = 2\epsilon$
\end{center}
Because we want to know the probability that all of the $M$ dimensions of $x - y$ are between $-\epsilon$ and $\epsilon$, we simply take $\Pi_{i = 1}^{M} P(|x_i - y_i| \leq \epsilon) = (2\epsilon)^M$.
\item The probability of $max_m |x_m - y_m| \leq \epsilon$ is at most $\rho$ because as shown in (a), $\rho$ does not depend on $x_i$ and thus holds for all $x_i$. In addition, logically, if $x$ is the center point, the average distance from it to any other point $y$ is at most $\frac{1}{2}$ for any one dimension. As $x$ moves farther and farther away from the center, the average distance increases so that it becomes at most 1 in any one dimension. So, if $x$ is not in the center $max_m|x_m - y_m|$ grows and is less likely to be less than $\epsilon$, decreasing that probability so that it is less than $\rho$.
\item We will show that $||x - y|| \geq max_m|x_m - y_m|$.
\begin{center}
$||x - y|| = \sqrt{\Sigma_{m = 1}^{M} (x_m - y_m)^2} $ \\
$\sqrt{\Sigma_{m = 1}^{M} (x_m - y_m)^2} \geq max_m|x_m - y_m|$ \\
$\Sigma_{m = 1}^{M} (x_m - y_m)^2 \geq (max_m|x_m - y_m|)^2$
\end{center}
This is true because the left side of the inequality includes the right side in its sum. $|| x - y ||$ is the total Euclidean distance between two points whereas $max_m|x_m - y_m|$ is only the distance between one dimension of two points. The left side must be larger.
\newline \newline
If $x$ is any point in $\chi$, and $y$ is a point in $\chi$ drawn randomly from a uniform distribution on $\chi$, then the probabilty that $||x - y|| \leq \epsilon$ is also at most $p$ because $||x - y||$ is greater than or equal to $max_m|x_m - y_m|$, making it less likely to be less than $\epsilon$ and thus giving it a probability lower than $\rho$ of being less than $\epsilon$.
\item Lowerbound on number $N$ of points needed to guarantee that the nearest neighbor of point $x$
 will be within a radius $\epsilon$ of it is \fbox{$log\delta / log(1 - (2\epsilon)^M)$}.
\newline
For the nearest neighbor not to be within a radius $\epsilon$, none of the neighbors can be within a radius $\epsilon$. The probability that any one neighbor is not within a radius $\epsilon$ of $x$ is $1 - (2\epsilon)^M$, so the probability that all the nighbors are not within a radius $\epsilon$ of $x$ is equivalent to $(1 - (2\epsilon)^M)^N$, where $N$ is the number of neighbors. So, the probability that at least one neighbor is within a radius $\epsilon$ is 1 - that quantity. Since we want to guarantee with probability at least $1 - \delta$ that the nearest neighbor will be within a radius $\epsilon$ of it, we can solve for a lower bound for N by setting the two equations equal to each other.
\begin{center}
$1 - \delta = 1 - (1 - (2\epsilon)^M)^N$ \\
$1 - 1 + (1 - (2\epsilon)^M)^N = \delta$ \\
$(1 - (2\epsilon)^M)^N = \delta $ \\
$Nlog(1 - (2\epsilon)^M) = log\delta$ \\
\fbox{$N = log\delta/log(1 - (2\epsilon)^M)$}
\end{center}
\item We can conclude that the effectiveness of the hierarchical agglomerative clustering algorithm in high dimensional spaces is ineffective as the dimension $M$ grows because $N$ would also grow too large and HAC would require too many $N$ points to actually be effective. As $M$ increases, the denominator of the lower bound for $N$ decreases, thus leading to an increase in $N$ overall. In addition, as covered in class, when the size of the dataset gets larger, the probability that two points from different clusters are closer to each other in terms of distance than two points from separate clusters converges to 1/2.
\end{enumerate}

\section{Problem 2}
\begin{enumerate}[a.]
\item Given a prior distribution $Pr(\theta)$ and likelihood $Pr(D|\theta)$, the predictive distribution $Pr(x|D)$ for a new datum,
\begin{enumerate}
\item ML: $Pr(x|D)=Dist(\underset{\theta}{\arg\max}(\ln(Pr(D|\theta))))$, \\where $Dist()$ is a distribution applied to the $\theta$ we obtain with the given formula.
\item MAP: $Pr(x|D)=Dist(\underset{\theta}{\arg\max}(\ln(P(D|\theta)P(\theta))))$, \\where $Dist()$ is a distribution applied to the $\theta$ we obtain with the given formula.
\item FB: $Pr(x|D)=\int p(x|\theta) P(\theta|D)d\theta$
\end{enumerate}
\item MAP can be considered "more Bayesian" than ML because it takes into account the distribution of $\theta$ instead of assuming same weight or uniformity.
\item One advantage the MAP method enjoys over the ML method is that it accounts for the more likely distribution, as opposed to simply assuming uniformity, as with ML. FB, on the other hand, maintains a probability distribution is maintained over the set of all parameter values possible. However, because the normalizing factor contains an integration over all parameter values, it can be difficult to compute. This means that it also unnecessarily takes into account the less likely, meaning that FB is less practical than MAP to calculate. Therefore, MAP sits in the "sweet spot" of taking more into account in terms of distribution than ML but less excessively than FB.
\item Soccer team example based on different Beta distributions as priors for the probability of a win, the different possible distributions are:
\begin{itemize}
\item Beta(1,1)\\
\includegraphics[width=80mm]{beta11.png}\\
The Beta(1,1) distribution is equated with the Uniform(0,1) distribution, which is a number uniformly chosen between 0 and 1. Beta(1,1) is a uniform prior on $\theta$ which says there was just one positive and one negative example, essentially makes the probability of winning and losing equally likely. Often used as a "non-informative prior", this assumes uniformity and is not very descriptive in depicting the distribution of wins and losses otherwise.
\item Beta(3,3)\\
\includegraphics[width=80mm]{beta33.png}\\
Based on the equation for the mode of a variate with a beta distribution, $x = \frac{\alpha - 1}{\alpha + \beta - 2}=\frac{3-1}{3+3-2}=\frac{2}{4}$. This means that this distribution says that it is as likely to win as it is to lose as the distribution is symmetrical. However, this is more descriptive than simply a Beta(1,1) distribution, as seen in the plot because it looks more at the distribution over the entire space, with a lower variance as it makes values in the middle more likely.
\item Beta(2,5)\\
\includegraphics[width=80mm]{beta25.png}\\
In this case, the plot seems to be skewed toward the left, which the equation for the mode of a variate with a beta distribution show $x = \frac{\alpha - 1}{\alpha + \beta - 2}=\frac{2-1}{2+5-2}=\frac{1}{5}$ that this is true. This is more descriptive than Beta(1,1) in describing a distribution that is not necessarily uniform and it is also more more descriptive than Beta(3,3) in describing a distribution that is not necessarily as symmetrical.
\end{itemize}
\item The Beta distribution is the conjugate prior of the Bernoulli.
\item Under the ML approach
\end{enumerate}

\section{Problem 3}
\begin{enumerate}[a.]
\item The $K$ -means clustering objective is to minimize the sum of squared distances between prototype and data.
\item PCA relates to $K$-means
\end{enumerate}

\section{Problem 4}
\begin{enumerate}[a.]
\item $K$-means clustering algorithm
\item HAC algorithm
\begin{enumerate}
\item Comparing clusters formed using $min$ distance metric against clusters formed using $max$ distance metric
\begin{enumerate}
\item Table showing number of instances in each cluster\\
\begin{tabular}{|c|c|c|c|c|}
\hline
Metric & C1 & C2 & C3 & C4\\ \hline
$min$ & 1 & 1 & 73 & 25\\ \hline
$max$ & 7 & 21 & 46 & 26\\
\hline
\end{tabular}
\item Scatterplot of the instances in 3-dimensions
	\begin{itemize}
	\item Based on $min$:\\
	\includegraphics[width=120mm]{graphhac.png}
	\item Based on $max$:\\
	\includegraphics[width=120mm]{graphhacmax.png}
	\end{itemize}
\end{enumerate}
It seems that the $min$ distance metric produced ?? clusters compared to the $max$ metric... This makes sense given the definition of the metrics because...
\item Comparing clusters formed using $mean$ distance metric against clusters formed using $centroid$ distance metric
\begin{enumerate}
\item Table showing number of instances in each cluster\\
\begin{tabular}{|c|c|c|c|c|}
\hline
Metric & C1 & C2 & C3 & C4\\ \hline
$mean$ & 1 & 1 & 46 & 152\\ \hline
$cent$ & 1 & 5 & 147 & 47\\
\hline
\end{tabular}
\item Scatterplot of the instances in 3-dimensions
\begin{itemize}
	\item Based on $mean$:\\
	\includegraphics[width=120mm]{graphhacmean.png}
	\item Based on $cent$:\\
	\includegraphics[width=120mm]{graphhaccent.png}
	\end{itemize}
\end{enumerate}
It seems that the $mean$ distance metric produced ?? clusters compared to the $cent$ metric... This makes sense given the definition of the metrics because...
\end{enumerate}

\item Autoclass clustering algorithm
\end{enumerate}

\end{document}