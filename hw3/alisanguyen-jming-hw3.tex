\documentclass[11pt]{article}
\usepackage{latexsym}
\usepackage{amssymb,amsmath}
\usepackage[pdftex]{graphicx}
% \usepackage{qtree}

\topmargin = -.75in \textwidth=7in \textheight=9.3in
\oddsidemargin = -.3in \evensidemargin = -.3in

\begin{document}
\begin{center}
\large
CS181 Assignment 3
\end{center}
Joy Ming and Alisa Nguyen (15 March 2013)\\

\section{Problem 1}
\begin{enumerate}
\item The probability that all of the $M$ dimensions of $x - y$ are between $-\epsilon \text{ and } \epsilon$ is \fbox{$(2\epsilon)^M$}.
\\ For each dimension $i$ of $\chi$, the probability that $|x_i - y_i| \leq \epsilon$ is equivalent to 
\begin{center}
$P(|x_i - y_i| \leq \epsilon) = $ \\
$P(- \epsilon \leq x_i - y_i \leq \epsilon) = $ \\
$P(- \epsilon - x_i \leq - y_i \leq \epsilon - x_i) = $ \\
$P(\epsilon + x_i \geq y_i \geq x_i - \epsilon) = $ \\
$P(x_i - \epsilon \leq y_i \leq \epsilon + x_i)$
\end{center}
This distribution function is equivalent to $\int_{\epsilon + x_i}^{x_i - \epsilon} f(x)dx$, where $f(x)$ is the PDF of $y_i$, which we know to have a uniform distribution, so $f(x) = \frac{1}{b - a} = 1$. Thus, we get:
\begin{center}
$\int_{\epsilon + x_i}^{x_i - \epsilon} 1 dx = $ \\
$\epsilon + x_i - (x_i - \epsilon) = 2\epsilon$
\end{center} 
Because we want to know the probability that all of the $M$ dimensions of $x - y$ are between $-\epsilon$ and $\epsilon$, we simply take $\Pi_{i = 1}^{M} P(|x_i - y_i| \leq \epsilon) = (2\epsilon)^M$.
\item The probability of $max_m |x_m - y_m| \leq \epsilon$ is at most $p$
\item If $x$ is any point in $\chi$, and $y$ is a point in $\chi$ drawn randomly from a uniform distribution on $\chi$, then the probabilty that $||x - y|| \leq \epsilon$ is also at most $p$
\item Lowerbound on number $N$ of points needed to guarantee
\item We can conclude that the effectiveness of the hierarchical agglomerativ clustering algorithm in high dimensional spaces
\end{enumerate}

\section{Problem 2}
\begin{enumerate}
\item Given a prior distribution $Pr(\theta)$ and likelihood $Pr(D|\theta)$, the predictive distribution $Pr(x|D)$ for a new datum, 
	\begin{enumerate}
	\item ML: $Pr(x|D)=Pr(x|\theta)=\underset{\theta}{\arg\max}(\ln(Pr(D|\theta)))$
	\item MAP: $Pr(x|D)=Pr(x|D)=Pr(x|\theta)=\underset{\theta}{\arg\max}(\ln(P(D|\theta)P(\theta)))$
	\item FB: $Pr(x|D)=\int \theta P(\theta|D)d\theta$
	\end{enumerate}
\item MAP can be considered "more Bayesian" than ML because it takes into account the distribution of $\theta$ instead of assuming same weight or uniformity.
\item One advantage the MAP method enjoys over the ML method
\item The Beta distribution is the conjugate prior of the Bernoulli.
\item Under the ML approach
\end{enumerate}

\section{Problem 3}
\begin{enumerate}
\item The $K$ -means clustering objective is to minimize the sum of squared distances between prototype and data.
\item PCA relates to $K$-means
\end{enumerate}

\section{Problem 4}

\end{document}